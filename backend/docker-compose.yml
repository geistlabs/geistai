# Test Docker GPU access with: docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi

services:
  # CPU mode router service
  router:
    build: ./router
    ports:
      - "0.0.0.0:8000:8000"  # Bind to all interfaces
      - "0.0.0.0:8443:8443"# HTTPS port (uncomment if using SSL)
    environment:
      - LOG_LEVEL=DEBUG
      - HARMONY_REASONING_EFFORT=low
      - INFERENCE_URL=http://inference:8080
      - EMBEDDINGS_URL=http://embeddings:8001
      # Development-specific Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - WATCHDOG_POLLING=true

      - OPENAI_URL=https://api.openai.com
      - USE_REMOTE_INFERENCE=${USE_REMOTE_INFERENCE}
      - OPENAI_KEY=${OPENAI_KEY}
      - USE_REMOTE_INFERENCE=true
      - BRAVE_API_KEY=${BRAVE_API_KEY}
      - MCP_BRAVE_URL=http://mcp-brave:8080
      - MCP_FETCH_URL=http://mcp-fetch:8000

    volumes:
      # Mount source code for live reloading
      - ./router:/app
      # Mount certificates directory if needed (read-only is fine for certs)
      - ./router/certificates:/app/certificates:ro
    command:
      [
        "python",
        "-c",
        "import uvicorn; uvicorn.run('main:app', host='0.0.0.0', port=8000, reload=True, reload_dirs=['/app'])",
      ]
    depends_on:
      - inference
      - mcp-brave
      - mcp-fetch
      - embeddings
    healthcheck:
      # Use HTTP for healthcheck in local development
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Development settings
    restart: "no"
    labels:
      - "dev.environment=development"
      - "dev.service=router"
    networks:
      - geist-network
    profiles:
      - cpu

  # GPU mode router service
  router-gpu:
    build: ./router
    ports:
      - "8000:8000"  # Bind to all interfaces
      - "8443:8443"# HTTPS port (uncomment if using SSL)
    environment:
      - LOG_LEVEL=DEBUG
      - HARMONY_REASONING_EFFORT=low
      - INFERENCE_URL=http://inference-gpu:8080
      - EMBEDDINGS_URL=http://embeddings:8001
      # Development-specific Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - WATCHDOG_POLLING=true
      - MCP_BRAVE_URL=http://mcp-brave:8080
      - MCP_FETCH_URL=http://mcp-fetch:8000
      - OPENAI_URL=https://api.openai.com
      - USE_REMOTE_INFERENCE=${USE_REMOTE_INFERENCE}
      - OPENAI_KEY=${OPENAI_KEY}
      - USE_REMOTE_INFERENCE=true
      - BRAVE_API_KEY=${BRAVE_API_KEY}

    volumes:
      # Mount source code for live reloading
      - ./router:/app
      # Mount certificates directory if needed (read-only is fine for certs)
      - ./router/certificates:/app/certificates:ro
    command:
      [
        "python",
        "-c",
        "import uvicorn; uvicorn.run('main:app', host='0.0.0.0', port=8000, reload=True, reload_dirs=['/app'])",
      ]
    depends_on:
      - inference
      - mcp-brave
      - mcp-fetch
    healthcheck:
      # Use HTTP for healthcheck in local development
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Development settings
    restart: "no"
    labels:
      - "dev.environment=development"
      - "dev.service=router-gpu"
    networks:
      - geist-network
    profiles:
      - gpu

  # Local inference router service (assumes inference runs on host)
  router-local:
    build: ./router
    ports:
      - "8000:8000"  # Bind to all interfaces
      - "8443:8443"  # HTTPS port (uncomment if using SSL)
    environment:
      - LOG_LEVEL=DEBUG
      - HARMONY_REASONING_EFFORT=low
      - INFERENCE_URL=http://host.docker.internal:8080  # Connect to host inference
      - EMBEDDINGS_URL=http://embeddings:8001
      # Development-specific Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - WATCHDOG_POLLING=true
      - MCP_BRAVE_URL=http://mcp-brave:8080
      - MCP_FETCH_URL=http://mcp-fetch:8000
      - OPENAI_URL=https://api.openai.com
      - USE_REMOTE_INFERENCE=false
      - OPENAI_KEY=${OPENAI_KEY}
      - BRAVE_API_KEY=${BRAVE_API_KEY}

    volumes:
      # Mount source code for live reloading
      - ./router:/app
      # Mount certificates directory if needed (read-only is fine for certs)
      - ./router/certificates:/app/certificates:ro
    command:
      [
        "python",
        "-c",
        "import uvicorn; uvicorn.run('main:app', host='0.0.0.0', port=8000, reload=True, reload_dirs=['/app'])",
      ]
    depends_on:
      - mcp-brave
      - mcp-fetch
      - embeddings
    healthcheck:
      # Use HTTP for healthcheck in local development
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Development settings
    restart: "no"
    labels:
      - "dev.environment=development"
      - "dev.service=router-local"
    networks:
      - geist-network
    profiles:
      - local

  gateway:
    image: docker/mcp-gateway
    ports:
      - "9011:9011"
    command:
      - --transport=http
      - --servers=brave,fetch
      - --verbose=true
      - --port=9011
      - --static=true
      - --log-calls
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "4G"
        reservations:
          cpus: "1.0"
          memory: "2G"
    networks:
      - geist-network
    environment:
      - BRAVE_API_KEY=${BRAVE_API_KEY}
    depends_on:
      - mcp-brave
      - mcp-fetch


  mcp-brave:
    image: mcp/brave-search:latest

    environment:
      - BRAVE_API_KEY=${BRAVE_API_KEY}
      - BRAVE_MCP_TRANSPORT=http
      - MCP_PORT=8080
      - PORT=8080
    init: true
    cpus: 1
    mem_limit: 2g
    security_opt:
      - no-new-privileges
    labels:
      - docker-mcp=true
      - docker-mcp-tool-type=mcp
      - docker-mcp-name=brave
      - docker-mcp-transport=http
    ports:
      - "3001:8080"  # Expose MCP service on port 3001
    networks:
      - geist-network

  mcp-fetch:
    image: mcp/fetch:latest
    entrypoint: ["sh", "-c"]
    command: [". /app/.venv/bin/activate && pip install mcp-http-bridge && mcp-http-bridge --command 'python -m mcp_server_fetch' --port 8000 --host 0.0.0.0"]
    environment:
    - PORT=8000
    - FETCH_MCP_TRANSPORT=http
    init: true
    cpus: 1
    mem_limit: 2g
    security_opt:
      - no-new-privileges
    labels:
      - docker-mcp=true
      - docker-mcp-tool-type=mcp
      - docker-mcp-name=fetch
      - docker-mcp-transport=http
    ports:
      - "3002:8000"  # Expose MCP service on port 3002
    networks:
      - geist-network

  # CPU-only inference service (default)
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile.cpu
    ports:
      - "8080:8080"
    platform: linux/amd64
    environment:
      - MODEL_PATH=./models/gpt-oss-20b-Q4_K_S.gguf
      - HOST=0.0.0.0
      - PORT=8080
      - CONTEXT_SIZE=4096
      - THREADS=0 # Auto-detect CPU threads
      - GPU_LAYERS=0 # CPU-only mode
    deploy:
      resources:
        limits:
          memory: 16G # Adjust based on model size (GPT-OSS 20B needs more)
        reservations:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s # Model loading takes time
    volumes:
      # Mount the models directory
      - ./inference/models:/models:ro
    # Apple Silicon M3 GPU acceleration happens automatically via Metal backend
    restart: unless-stopped
    labels:
      - "dev.environment=development"
      - "dev.service=inference"
    networks:
      - geist-network
    profiles:
      - cpu

  # GPU-enabled inference service (requires nvidia-docker)
  inference-gpu:
    build:
      context: ./inference
      dockerfile: Dockerfile.gpu
    ports:
      - "8080:8080"
    platform: linux/amd64 # Use AMD64 for NVIDIA GPU support
    environment:
      - MODEL_PATH=./models/gpt-oss-20b-Q4_K_S.gguf
      - HOST=0.0.0.0
      - PORT=8080
      - CONTEXT_SIZE=4096
      - THREADS=0 # Auto-detect CPU threads
      - GPU_LAYERS=${GPU_LAYERS:-8} # Use 8 GPU layers for RTX 5070 (8GB VRAM)
    deploy:
      resources:
        limits:
          memory: 16G # Adjust based on model size (GPT-OSS 20B needs more)
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s # Model loading takes time
    volumes:
      # Mount the models directory
      - ./inference/models:/models:ro
    restart: unless-stopped
    labels:
      - "dev.environment=development"
      - "dev.service=inference-gpu"
    networks:
      - geist-network
    profiles:
      - gpu

  embeddings:
    build:
      context: ./embeddings
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    ports:
      - "8001:8001"
    environment:
      - ENVIRONMENT=development
      - LOG_LEVEL=DEBUG
      - API_HOST=0.0.0.0
      - API_PORT=8001
      - DEFAULT_MODEL=all-MiniLM-L6-v2
      - MODEL_CACHE_DIR=/opt/venv/models
      - SENTENCE_TRANSFORMERS_HOME=/opt/venv/models
      # Development-specific Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - WATCHDOG_POLLING=true
    volumes:
      # Mount source code for live reloading (read-write to allow model directory creation)
      - ./embeddings:/app
      # Keep model cache volume for performance using the optimized cache location
      - embeddings-models:/opt/venv/models
    command:
      [
        "uvicorn",
        "main:app",
        "--host",
        "0.0.0.0",
        "--port",
        "8001",
        "--reload",
        "--reload-dir",
        "/app",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    # Development settings
    restart: "no"
    labels:
      - "dev.environment=development"
      - "dev.service=embeddings"
    networks:
      - geist-network

  # CPU mode webapp service
  webapp:
    build: ./webapp
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
      - NODE_ENV=development
    volumes:
      # Mount source code for live reloading
      - ./webapp/src:/app/src
      - ./webapp/public:/app/public
      - ./webapp/index.html:/app/index.html
      - ./webapp/vite.config.ts:/app/vite.config.ts
      - ./webapp/tsconfig.json:/app/tsconfig.json
      - ./webapp/tsconfig.node.json:/app/tsconfig.node.json
    depends_on:
      - router
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      - "dev.environment=development"
      - "dev.service=webapp"
    networks:
      - geist-network
    profiles:
      - cpu

  # GPU mode webapp service
  webapp-gpu:
    build: ./webapp
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
      - NODE_ENV=development
    volumes:
      # Mount source code for live reloading
      - ./webapp/src:/app/src
      - ./webapp/public:/app/public
      - ./webapp/index.html:/app/index.html
      - ./webapp/vite.config.ts:/app/vite.config.ts
      - ./webapp/tsconfig.json:/app/tsconfig.json
      - ./webapp/tsconfig.node.json:/app/tsconfig.node.json
    depends_on:
      - router-gpu
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      - "dev.environment=development"
      - "dev.service=webapp-gpu"
    networks:
      - geist-network
    profiles:
      - gpu

volumes:
  embeddings-models:
    driver: local

networks:
  geist-network:
    driver: bridge
