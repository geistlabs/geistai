FROM ghcr.io/ggml-org/llama.cpp:server

# Create models directory for volume mount
USER root
RUN mkdir -p /models && chown 1000:1000 /models

# Switch back to non-root user
USER 1000

EXPOSE 8080

# CPU-only configuration - no GPU arguments
# Use environment variables for configuration
CMD /app/llama-server \
		--model "${MODEL_PATH:-/models/gpt-oss-20b-Q4_K_S.gguf}" \
		--host "${HOST:-0.0.0.0}" \
		--port "${PORT:-8080}" \
		--ctx-size "${CONTEXT_SIZE:-4096}" \
		--threads "${THREADS:-0}" \
		--batch-size 512 \
		--ubatch-size 512 \
		--parallel 1 \
		--cont-batching \
		--mlock

# Example usage:
# docker run -d \
#   --name inference-server-cpu \
#   -p 8080:8080 \
#   -v /root/models:/models:ro \
#   alo42/inference:cpu
