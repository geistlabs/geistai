FROM ghcr.io/ggml-org/llama.cpp:server-cuda

# Create models directory for volume mount
USER root

# Use platform-specific approach to avoid ARM64 emulation issues
RUN --mount=type=cache,target=/var/cache/apt \
    if [ "$(uname -m)" = "aarch64" ]; then \
        # For ARM64, use simpler commands that work better with QEMU
        mkdir -p /models; \
        chown 1000:1000 /models; \
    else \
        # For AMD64, use the original command
        mkdir -p /models && chown 1000:1000 /models; \
    fi

# Create entrypoint script to handle environment variables properly with JSON CMD
RUN echo '#!/bin/sh\n\
exec /app/llama-server \\\n\
    --model "${MODEL_PATH:-/models/gpt-oss-20b-Q4_K_S.gguf}" \\\n\
    --host "${HOST:-0.0.0.0}" \\\n\
    --port "${PORT:-8080}" \\\n\
    --ctx-size "${CONTEXT_SIZE:-4096}" \\\n\
    --threads "${THREADS:-4}" \\\n\
    --batch-size 512 \\\n\
    --ubatch-size 512 \\\n\
    --parallel 1 \\\n\
    --cont-batching \\\n\
    --mlock \\\n\
    --n-gpu-layers "${GPU_LAYERS:-8}"' > /entrypoint.sh && \
    chmod +x /entrypoint.sh && \
    chown 1000:1000 /entrypoint.sh

# Switch back to non-root user
USER 1000

EXPOSE 8080

# Use JSON array format for CMD to prevent signal handling issues
CMD ["/entrypoint.sh"]

# Example usage:
# docker run -d \
#   --name inference-server \
#   --gpus all \
#   -p 8080:8080 \
#   -v /root/models:/models:ro \
#   alo42/inference:latest


# In order to use the GPU, we need to install the nvidia-container-toolkit on the host machine.

		# Add the package repositories
# distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
# curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
# curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# # Install and restart Docker
# sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
# sudo systemctl restart docker

# You can test Docker GPU access with this
# docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi