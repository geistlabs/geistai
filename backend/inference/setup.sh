  #!/bin/bash
  # Download llama.cpp server binary (pre-built)
  # We'll use a small model for testing
  
    echo "Setting up llama.cpp inference server..."