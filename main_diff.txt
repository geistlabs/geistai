--- backend/router/main.py	2025-10-24 19:13:59
+++ backend/router/main.py.backup	2025-10-24 19:13:55
@@ -1,5 +1,5 @@
 from events import EventEmitter
-from fastapi import FastAPI, Request, UploadFile, File, Form, HTTPException
+from fastapi import FastAPI, Request, UploadFile, File, Form, HTTPException, Depends
 from fastapi.responses import StreamingResponse
 from fastapi.middleware.cors import CORSMiddleware
 from sse_starlette.sse import EventSourceResponse
@@ -16,6 +16,8 @@
 from agent_registry import get_predefined_agents
 from prompts import get_prompt, get_summarizer_prompt
 from chat_types import ChatMessage
+from revenuecat_auth import require_premium, get_user_id
+from agent_tool import create_pricing_agent
 
 from whisper_client import WhisperSTTClient
 
@@ -84,7 +86,7 @@
     This allows us to reuse the same service instance but with different event emitters per request.
     """
     global gpt_service_instance
-    
+
     # Initialize the service once if not already done
     if gpt_service_instance is None:
         # Create a temporary event emitter for initialization
@@ -92,15 +94,15 @@
         gpt_service_instance = GptService(config, temp_emitter, can_log=True)
         await gpt_service_instance.init_tools()
         logger.info("GptService initialized with tools")
-    
+
     # Create a new instance with a fresh event emitter for each request
     fresh_emitter = EventEmitter()
     new_service = GptService(config, fresh_emitter, can_log=True)
-    
+
     # Copy the tool registry from the initialized instance
     new_service._tool_registry = gpt_service_instance._tool_registry.copy()
     new_service._mcp_client = gpt_service_instance._mcp_client
-    
+
     return new_service
 
 @app.on_event("startup")
@@ -365,168 +367,194 @@
         )
 
 
-@app.post("/api/stream")
-async def stream_with_orchestrator(chat_request: ChatRequest, request: Request):
-    """Enhanced streaming endpoint with orchestrator and sub-agent visibility"""
-    
-    gpt_service = await get_gpt_service()
-    # Build messages array with conversation history
-    messages = chat_request.messages
-    if not messages:
-        messages = [ChatMessage(role="user", content=chat_request.message)]
-    else:
-        messages.append(ChatMessage(role="user", content=chat_request.message))
+async def create_orchestrator_event_stream(
+    messages: List,
+    orchestrator,
+    gpt_service,
+    request: Request = None,
+    logger_prefix: str = "[Stream]"
+):
+    """
+    Generic orchestrator event streaming helper
 
-    async def orchestrator_event_stream():
-        orchestrator_task = None
-
-        try:
-            # Create a nested orchestrator structure
-            orchestrator = create_nested_research_system(config)
-
-            # Initialize the orchestrator with the main GPT service
-            await orchestrator.initialize(gpt_service, config)
-            orchestrator.gpt_service = gpt_service
-
-            # Use asyncio.Queue to stream events in real-time
-            event_queue = asyncio.Queue()
-            final_response = None
-            sequence_counter = {"value": 0}  # Use dict for mutable closure
+    Args:
+        messages: List of ChatMessage objects or dicts
+        orchestrator: The orchestrator instance to run
+        gpt_service: The GPT service instance
+        request: FastAPI Request object (optional, for disconnect detection)
+        logger_prefix: Prefix for logging messages
+    """
+    # Ensure messages are ChatMessage objects
+    if messages and isinstance(messages[0], dict):
+        messages = [ChatMessage(role=msg["role"], content=msg["content"]) for msg in messages]
 
-            def queue_event(event_type):
-                """Create event handler that queues events with proper sequencing"""
-                def handler(data):
-                    event_data = {
-                        "type": event_type,
-                        "data": data,
-                        "sequence": sequence_counter["value"]
-                    }
-                    sequence_counter["value"] += 1
-                    try:
-                        event_queue.put_nowait(event_data)
-                    except asyncio.QueueFull:
-                        logger.warning(f"Event queue full, dropping {event_type} event")
-                return handler
+    orchestrator_task = None
 
-            # Register event listeners - orchestrator handles sub-agent event propagation
-            orchestrator.on("orchestrator_start", queue_event("orchestrator_start"))
-            orchestrator.on("agent_token", queue_event("orchestrator_token"))
-            orchestrator.on("orchestrator_complete", queue_event("orchestrator_complete"))
-            orchestrator.on("sub_agent_event", queue_event("sub_agent_event"))
-            orchestrator.on("tool_call_event", queue_event("tool_call_event"))
+    try:
+        # Initialize the orchestrator with the GPT service
+        await orchestrator.initialize(gpt_service, config)
+        orchestrator.gpt_service = gpt_service
 
-            # Start orchestrator in background
-            orchestrator_task = asyncio.create_task(orchestrator.run(messages))
+        # Use asyncio.Queue to stream events in real-time
+        event_queue = asyncio.Queue()
+        sequence_counter = {"value": 0}
 
-            # Stream events as they come in
-            while True:
+        def queue_event(event_type):
+            """Create event handler that queues events with proper sequencing"""
+            def handler(data):
+                event_data = {
+                    "type": event_type,
+                    "data": data,
+                    "sequence": sequence_counter["value"]
+                }
+                sequence_counter["value"] += 1
                 try:
-                    # Wait for either an event or orchestrator completion
-                    done, pending = await asyncio.wait(
-                        [
-                            asyncio.create_task(event_queue.get()),
-                            orchestrator_task
-                        ],
-                        return_when=asyncio.FIRST_COMPLETED
-                    )
+                    event_queue.put_nowait(event_data)
+                except asyncio.QueueFull:
+                    logger.warning(f"{logger_prefix} Event queue full, dropping {event_type} event")
+            return handler
 
-                    # Check if orchestrator is done
-                    if orchestrator_task in done:
-                        final_response = await orchestrator_task
+        # Register event listeners
+        # Support both orchestrator events and agent events (for direct AgentTool usage)
+        orchestrator.on("orchestrator_start", queue_event("orchestrator_start"))
+        orchestrator.on("agent_start", queue_event("orchestrator_start"))  # Map agent_start to orchestrator_start
+        orchestrator.on("agent_token", queue_event("orchestrator_token"))
+        orchestrator.on("orchestrator_complete", queue_event("orchestrator_complete"))
+        orchestrator.on("agent_complete", queue_event("orchestrator_complete"))  # Map agent_complete to orchestrator_complete
+        orchestrator.on("sub_agent_event", queue_event("sub_agent_event"))
+        orchestrator.on("tool_call_event", queue_event("tool_call_event"))
 
-                        # Cancel pending event queue task
-                        for task in pending:
-                            task.cancel()
+        # Start orchestrator in background
+        orchestrator_task = asyncio.create_task(orchestrator.run(messages))
 
-                        # Drain remaining events from queue
-                        while not event_queue.empty():
-                            try:
-                                event = event_queue.get_nowait()
-                                if await request.is_disconnected():
-                                    return
+        # Stream events as they come in
+        while True:
+            try:
+                # Wait for either an event or orchestrator completion
+                done, pending = await asyncio.wait(
+                    [
+                        asyncio.create_task(event_queue.get()),
+                        orchestrator_task
+                    ],
+                    return_when=asyncio.FIRST_COMPLETED
+                )
 
-                                yield {
-                                    "data": json.dumps(event),
-                                    "event": event.get("type", "unknown")
-                                }
-                            except asyncio.QueueEmpty:
-                                break
-                        break
+                # Check if orchestrator is done
+                if orchestrator_task in done:
+                    final_response = await orchestrator_task
+                    logger.info(f"{logger_prefix} Orchestrator completed")
 
-                    # Process events
-                    for task in done:
-                        if task != orchestrator_task:
-                            event = await task
-                            if await request.is_disconnected():
-                                logger.info("Client disconnected, stopping stream")
-                                orchestrator_task.cancel()
+                    # Cancel pending event queue task
+                    for task in pending:
+                        task.cancel()
+
+                    # Drain remaining events from queue
+                    while not event_queue.empty():
+                        try:
+                            event = event_queue.get_nowait()
+                            if request and await request.is_disconnected():
                                 return
 
-                            if isinstance(event, dict):
-                                yield {
-                                    "data": json.dumps(event),
-                                    "event": event.get("type", "unknown")
-                                }
+                            yield {
+                                "data": json.dumps(event),
+                                "event": event.get("type", "unknown")
+                            }
+                        except asyncio.QueueEmpty:
+                            break
+                    break
 
-                    # Cancel pending event queue tasks (not the orchestrator)
-                    for task in pending:
-                        if task != orchestrator_task:
-                            task.cancel()
+                # Process events
+                for task in done:
+                    if task != orchestrator_task:
+                        event = await task
+                        if request and await request.is_disconnected():
+                            logger.info(f"{logger_prefix} Client disconnected, stopping stream")
+                            orchestrator_task.cancel()
+                            return
 
-                except asyncio.CancelledError:
-                    logger.info("Stream cancelled")
-                    break
+                        if isinstance(event, dict):
+                            yield {
+                                "data": json.dumps(event),
+                                "event": event.get("type", "unknown")
+                            }
+            except asyncio.TimeoutError:
+                pass
+            except asyncio.CancelledError:
+                logger.info(f"{logger_prefix} Stream cancelled")
+                break
 
-            # Send final response
-            if final_response:
-                yield {
-                    "data": json.dumps({
-                        "type": "final_response",
-                        "text": final_response.text,
-                        "status": final_response.status,
-                        "meta": final_response.meta,
-                        "sequence": sequence_counter["value"]
-                    }),
-                    "event": "final_response"
-                }
-                sequence_counter["value"] += 1
+    except Exception as e:
+        logger.error(f"{logger_prefix} Orchestrator error: {str(e)}")
+        import traceback
+        traceback.print_exc()
+        yield {
+            "data": json.dumps({
+                "type": "error",
+                "data": {"message": str(e)}
+            }),
+            "event": "error"
+        }
+    finally:
+        if orchestrator_task and not orchestrator_task.done():
+            orchestrator_task.cancel()
 
-            # Send end event
-            yield {
-                "data": json.dumps({
-                    "finished": True,
-                    "sequence": sequence_counter["value"]
-                }),
-                "event": "end"
-            }
+@app.post("/api/stream")
+async def stream_with_orchestrator(
+    chat_request: ChatRequest,
+    request: Request,
+    premium_status: dict = Depends(require_premium)
+):
+    """Enhanced streaming endpoint with orchestrator and sub-agent visibility
 
-        except asyncio.TimeoutError:
-            logger.error("Request timeout")
-            yield {
-                "data": json.dumps({"error": "Request timeout"}),
-                "event": "error"
-            }
-        except Exception as e:
-            logger.error(f"Stream error: {str(e)}", exc_info=True)
-            yield {
-                "data": json.dumps({
-                    "error": "Internal server error",
-                    "details": str(e)
-                }),
-                "event": "error"
-            }
-        finally:
-            # Cleanup: cancel orchestrator task if still running
-            if orchestrator_task and not orchestrator_task.done():
-                logger.info("Cleaning up orchestrator task")
-                orchestrator_task.cancel()
-                try:
-                    await orchestrator_task
-                except asyncio.CancelledError:
-                    pass
+    Requires: Premium subscription (checked via X-User-ID header)
+    """
+    logger.info(f"[Stream] Premium user accessing /api/stream: {premium_status}")
 
-    return EventSourceResponse(orchestrator_event_stream())
+    gpt_service = await get_gpt_service()
+    # Build messages array with conversation history
+    messages = chat_request.messages
+    if not messages:
+        messages = [ChatMessage(role="user", content=chat_request.message)]
+    else:
+        messages.append(ChatMessage(role="user", content=chat_request.message))
+
+    return EventSourceResponse(create_orchestrator_event_stream(messages, create_nested_research_system(config), gpt_service, request))
+
+
+@app.post("/api/negotiate")
+async def negotiate_pricing(
+    chat_request: ChatRequest,
+    request: Request,
+    user_id: str = Depends(get_user_id)
+):
+    """
+    Pricing negotiation endpoint using pricing agent directly (no orchestrator layer)
+    Uses the same pattern as /api/stream
+    """
+    logger.info(f"[Negotiate] User {user_id} starting pricing negotiation")
 
+    gpt_service = await get_gpt_service()
+
+    # Build messages array with conversation history
+    messages = chat_request.messages
+    if not messages:
+        messages = [ChatMessage(role="user", content=chat_request.message)]
+    else:
+        messages.append(ChatMessage(role="user", content=chat_request.message))
+
+    # Create pricing agent directly (not wrapped in an orchestrator)
+    pricing_agent = create_pricing_agent()
+
+    return EventSourceResponse(
+        create_orchestrator_event_stream(
+            messages=messages,
+            orchestrator=pricing_agent,  # Pass agent directly, not wrapped
+            gpt_service=gpt_service,
+            request=request,
+            logger_prefix="[Negotiate]"
+        )
+    )
+
 @app.post("/api/summarize")
 async def summarize_conversation(conversation_dict: List[dict]):
     """Summarize text using the orchestrator"""
@@ -534,8 +562,8 @@
     conversation_json = json.dumps(conversation_dict, ensure_ascii=False)
     content = f"{summarizer_prompt}\n\n[CONVERSATION_JSON]:\n{conversation_json}\n"
     conversation_dict = [{"role": "user", "content": content}]
-    
-    gpt_service = await get_gpt_service()  
+
+    gpt_service = await get_gpt_service()
     headers, model, url = gpt_service.get_chat_completion_params()
     async with httpx.AsyncClient() as client:
        response = await client.post(
